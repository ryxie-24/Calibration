{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77f61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 06:40:03.479941: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-18 06:40:07.981248: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-18 06:40:07.985318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-18 06:40:55.898470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi_res = 20.372\n",
      "['/usr/lib64/python39.zip', '/usr/lib64/python3.9', '/usr/lib64/python3.9/lib-dynload', '', '/eos/user/r/ruxie/tf_env/lib64/python3.9/site-packages', '/eos/user/r/ruxie/tf_env/lib/python3.9/site-packages', '/eos/home/r/ruxie/.local/lib/python3.9/site-packages']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eos/user/r/ruxie/tf_env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/eos/home/r/ruxie/.local/lib/python3.9/site-packages')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import colorsys\n",
    "\n",
    "from sklearn import metrics\n",
    "# from dcor import distance_correlation as dcor \n",
    "\n",
    "# phi resolution factor for mapping φ to pixel bins\n",
    "phi_res = 128 / (2 * math.pi) \n",
    "print(f\"phi_res = {phi_res:.3f}\")\n",
    "\n",
    "# Print paths to verify\n",
    "print(sys.path)\n",
    "\n",
    "from qkeras import QActivation, QDense, quantized_relu, quantized_bits\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import PReLU, Input, Concatenate, Dense, ReLU, Dropout, BatchNormalization, Activation\n",
    "from scipy.stats import pearsonr, linregress\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import colorsys\n",
    "\n",
    "\"\"\"Fold batchnormalization with previous QDense layers.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "\n",
    "\n",
    "from qkeras.qlayers import QDense\n",
    "from qkeras.quantizers import *\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.framework import smart_cond as tf_utils\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "tf.compat.v2.enable_v2_behavior()\n",
    "\n",
    "class QDenseBatchnorm(QDense):\n",
    "  \"\"\"Implements a quantized Dense layer fused with Batchnorm.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    kernel_quantizer=None,\n",
    "    bias_quantizer=None,\n",
    "    kernel_range=None,\n",
    "    bias_range=None,\n",
    "\n",
    "    # batchnorm params\n",
    "    axis=-1,\n",
    "    momentum=0.99,\n",
    "    epsilon=0.001,\n",
    "    center=True,\n",
    "    scale=True,\n",
    "    beta_initializer=\"zeros\",\n",
    "    gamma_initializer=\"ones\",\n",
    "    moving_mean_initializer=\"zeros\",\n",
    "    moving_variance_initializer=\"ones\",\n",
    "    beta_regularizer=None,\n",
    "    gamma_regularizer=None,\n",
    "    beta_constraint=None,\n",
    "    gamma_constraint=None,\n",
    "    renorm=False,\n",
    "    renorm_clipping=None,\n",
    "    renorm_momentum=0.99,\n",
    "    fused=None,\n",
    "    trainable=True,\n",
    "    virtual_batch_size=None,\n",
    "    adjustment=None,\n",
    "\n",
    "    # other params\n",
    "    ema_freeze_delay=None,\n",
    "    folding_mode=\"ema_stats_folding\",\n",
    "    **kwargs):\n",
    "\n",
    "    super(QDenseBatchnorm, self).__init__(\n",
    "      units=units,\n",
    "      activation=activation,\n",
    "      use_bias=use_bias,\n",
    "      kernel_initializer=kernel_initializer,\n",
    "      bias_initializer=bias_initializer,\n",
    "      kernel_regularizer=kernel_regularizer,\n",
    "      bias_regularizer=bias_regularizer,\n",
    "      activity_regularizer=activity_regularizer,\n",
    "      kernel_constraint=kernel_constraint,\n",
    "      bias_constraint=bias_constraint,\n",
    "      kernel_quantizer=kernel_quantizer,\n",
    "      bias_quantizer=bias_quantizer,\n",
    "      kernel_range=kernel_range,\n",
    "      bias_range=bias_range,\n",
    "      **kwargs)\n",
    "    \n",
    "    # initialization of batchnorm part of the composite layer\n",
    "    self.batchnorm = layers.BatchNormalization(\n",
    "      axis=axis, momentum=momentum, epsilon=epsilon, center=center,\n",
    "      scale=scale, beta_initializer=beta_initializer,\n",
    "      gamma_initializer=gamma_initializer,\n",
    "      moving_mean_initializer=moving_mean_initializer,\n",
    "      moving_variance_initializer=moving_variance_initializer,\n",
    "      beta_regularizer=beta_regularizer,\n",
    "      gamma_regularizer=gamma_regularizer,\n",
    "      beta_constraint=beta_constraint, gamma_constraint=gamma_constraint,\n",
    "      renorm=renorm, renorm_clipping=renorm_clipping,\n",
    "      renorm_momentum=renorm_momentum, fused=fused, trainable=trainable,\n",
    "      virtual_batch_size=virtual_batch_size, adjustment=adjustment\n",
    "      )\n",
    "\n",
    "    self.ema_freeze_delay = ema_freeze_delay\n",
    "    assert folding_mode in [\"ema_stats_folding\", \"batch_stats_folding\"]\n",
    "    self.folding_mode = folding_mode          \n",
    "\n",
    "  def build(self, input_shape):\n",
    "    super(QDenseBatchnorm, self).build(input_shape)\n",
    "\n",
    "    # self._iteration (i.e., training_steps) is initialized with -1. When\n",
    "    # loading ckpt, it can load the number of training steps that have been\n",
    "    # previously trainied. If start training from scratch.\n",
    "    # TODO(lishanok): develop a way to count iterations outside layer\n",
    "    self._iteration = tf.Variable(-1, trainable=False, name=\"iteration\",\n",
    "                                  dtype=tf.int64)\n",
    "\n",
    "  def call(self, inputs, training=None):\n",
    "\n",
    "    # numpy value, mark the layer is in training\n",
    "    training = self.batchnorm._get_training_value(training)  # pylint: disable=protected-access\n",
    "\n",
    "    # checking if to update batchnorm params\n",
    "    if (self.ema_freeze_delay is None) or (self.ema_freeze_delay < 0):\n",
    "      # if ema_freeze_delay is None or a negative value, do not freeze bn stats\n",
    "      bn_training = tf.cast(training, dtype=bool)\n",
    "    else:\n",
    "      bn_training = tf.math.logical_and(training, tf.math.less_equal(\n",
    "        self._iteration, self.ema_freeze_delay))\n",
    "\n",
    "    kernel = self.kernel\n",
    "    \n",
    "    #execute qdense output\n",
    "    qdense_outputs = tf.keras.backend.dot(\n",
    "      inputs, \n",
    "      kernel\n",
    "      )\n",
    "\n",
    "    if self.use_bias:\n",
    "      bias = self.bias\n",
    "      qdense_outputs = tf.keras.backend.bias_add(\n",
    "        qdense_outputs, \n",
    "        bias,\n",
    "        data_format=\"channels_last\")\n",
    "    else:\n",
    "      bias = 0\n",
    "\n",
    "    # begin batchnorm\n",
    "    _ = self.batchnorm(qdense_outputs, training=bn_training)\n",
    "\n",
    "    self._iteration.assign_add(tf_utils.smart_cond(\n",
    "        training, lambda: tf.constant(1, tf.int64),\n",
    "        lambda: tf.constant(0, tf.int64)))\n",
    "    \n",
    "    # calculate mean and variance from current batch\n",
    "    bn_shape = qdense_outputs.shape\n",
    "    ndims = len(bn_shape)\n",
    "    reduction_axes = [i for i in range(ndims) if i not in self.batchnorm.axis]\n",
    "    keep_dims = len(self.batchnorm.axis) > 1\n",
    "    mean, variance = self.batchnorm._moments(  # pylint: disable=protected-access\n",
    "        math_ops.cast(qdense_outputs, self.batchnorm._param_dtype),  # pylint: disable=protected-access\n",
    "        reduction_axes,\n",
    "        keep_dims=keep_dims)\n",
    "\n",
    "    # get batchnorm weights\n",
    "    gamma = self.batchnorm.gamma\n",
    "    beta = self.batchnorm.beta\n",
    "    moving_mean = self.batchnorm.moving_mean\n",
    "    moving_variance = self.batchnorm.moving_variance\n",
    "\n",
    "    if self.folding_mode == \"batch_stats_folding\":\n",
    "      # using batch mean and variance in the initial training stage\n",
    "      # after sufficient training, switch to moving mean and variance\n",
    "      new_mean = tf_utils.smart_cond(\n",
    "        bn_training, lambda: mean, lambda: moving_mean)\n",
    "      new_variance = tf_utils.smart_cond(\n",
    "        bn_training, lambda: variance, lambda: moving_variance)\n",
    "\n",
    "      # get the inversion factor so that we replace division by multiplication\n",
    "      inv = math_ops.rsqrt(new_variance + self.batchnorm.epsilon)\n",
    "      if gamma is not None:\n",
    "        inv *= gamma\n",
    "\n",
    "      # fold bias with bn stats\n",
    "      folded_bias = inv * (bias - new_mean) + beta\n",
    "\n",
    "    elif self.folding_mode == \"ema_stats_folding\":\n",
    "        # We always scale the weights with a correction factor to the long term\n",
    "        # statistics prior to quantization. This ensures that there is no jitter\n",
    "        # in the quantized weights due to batch to batch variation. During the\n",
    "        # initial phase of training, we undo the scaling of the weights so that\n",
    "        # outputs are identical to regular batch normalization. We also modify\n",
    "        # the bias terms correspondingly. After sufficient training, switch from\n",
    "        # using batch statistics to long term moving averages for batch\n",
    "        # normalization.\n",
    "\n",
    "        # use batch stats for calcuating bias before bn freeze, and use moving\n",
    "        # stats after bn freeze\n",
    "        mv_inv = math_ops.rsqrt(moving_variance + self.batchnorm.epsilon)\n",
    "        batch_inv = math_ops.rsqrt(variance + self.batchnorm.epsilon)\n",
    "\n",
    "        if gamma is not None:\n",
    "            mv_inv *= gamma\n",
    "            batch_inv *= gamma\n",
    "        folded_bias = tf_utils.smart_cond(\n",
    "          bn_training,\n",
    "          lambda: batch_inv * (bias - mean) + beta,\n",
    "          lambda: mv_inv * (bias - moving_mean) + beta)\n",
    "        # moving stats is always used to fold kernel in tflite; before bn freeze\n",
    "        # an additional correction factor will be applied to the conv2d output\n",
    "        # end batchnorm \n",
    "        inv = mv_inv\n",
    "    else:\n",
    "        assert ValueError\n",
    "\n",
    "    # wrap dense kernel with bn parameters\n",
    "    folded_kernel = inv*kernel\n",
    "    # quantize the folded kernel\n",
    "    if self.kernel_quantizer is not None:\n",
    "        q_folded_kernel = self.kernel_quantizer_internal(folded_kernel)\n",
    "    else:\n",
    "        q_folded_kernel = folded_kernel\n",
    "    \n",
    "    #quantize the folded bias\n",
    "    if self.bias_quantizer_internal is not None:\n",
    "        q_folded_bias = self.bias_quantizer_internal(folded_bias)\n",
    "    else:\n",
    "        q_folded_bias = folded_bias\n",
    "\n",
    "    applied_kernel = q_folded_kernel\n",
    "    applied_bias = q_folded_bias\n",
    "    \n",
    "    #calculate qdense output using the quantized folded kernel\n",
    "    folded_outputs = tf.keras.backend.dot(inputs, applied_kernel)\n",
    "\n",
    "    if training is True and self.folding_mode == \"ema_stats_folding\":\n",
    "      batch_inv = math_ops.rsqrt(variance + self.batchnorm.epsilon)\n",
    "      y_corr = tf_utils.smart_cond(\n",
    "          bn_training,\n",
    "          lambda: (math_ops.sqrt(moving_variance + self.batchnorm.epsilon) *\n",
    "                   math_ops.rsqrt(variance + self.batchnorm.epsilon)),\n",
    "          lambda: tf.constant(1.0, shape=moving_variance.shape))\n",
    "      folded_outputs = math_ops.mul(folded_outputs, y_corr)\n",
    "\n",
    "    folded_outputs = tf.keras.backend.bias_add(\n",
    "      folded_outputs, \n",
    "      applied_bias,\n",
    "      data_format=\"channels_last\"\n",
    "    )\n",
    "    \n",
    "    if self.activation is not None:\n",
    "      return self.activation(folded_outputs)\n",
    "    \n",
    "    return folded_outputs\n",
    "\n",
    "  def get_config(self):\n",
    "    base_config = super().get_config()\n",
    "    bn_config = self.batchnorm.get_config()\n",
    "    config = {\"ema_freeze_delay\": self.ema_freeze_delay,\n",
    "              \"folding_mode\": self.folding_mode}\n",
    "    name = base_config[\"name\"]\n",
    "    out_config = dict(\n",
    "        list(base_config.items())\n",
    "        + list(bn_config.items()) + list(config.items()))\n",
    "\n",
    "    # names from different config override each other; use the base layer name\n",
    "    # as the this layer's config name\n",
    "    out_config[\"name\"] = name\n",
    "    return out_config\n",
    "\n",
    "  def get_quantization_config(self):\n",
    "    return {\n",
    "        \"kernel_quantizer\": str(self.kernel_quantizer_internal),\n",
    "        \"bias_quantizer\": str(self.bias_quantizer_internal),\n",
    "    }\n",
    "    \n",
    "  def get_quantizers(self):\n",
    "    return self.quantizers\n",
    "\n",
    "  # def get_prunable_weights(self):\n",
    "  #   return [self.kernel]\n",
    "\n",
    "  def get_folded_weights(self):\n",
    "    \"\"\"Function to get the batchnorm folded weights.\n",
    "    This function converts the weights by folding batchnorm parameters into\n",
    "    the weight of QDense. The high-level equation:\n",
    "    W_fold = gamma * W / sqrt(variance + epsilon)\n",
    "    bias_fold = gamma * (bias - moving_mean) / sqrt(variance + epsilon) + beta\n",
    "    \"\"\"\n",
    "\n",
    "    kernel = self.kernel\n",
    "    if self.use_bias:\n",
    "      bias = self.bias\n",
    "    else:\n",
    "      bias = 0\n",
    "\n",
    "    # get batchnorm weights and moving stats\n",
    "    gamma = self.batchnorm.gamma\n",
    "    beta = self.batchnorm.beta\n",
    "    moving_mean = self.batchnorm.moving_mean\n",
    "    moving_variance = self.batchnorm.moving_variance\n",
    "\n",
    "    # get the inversion factor so that we replace division by multiplication\n",
    "    inv = math_ops.rsqrt(moving_variance + self.batchnorm.epsilon)\n",
    "    if gamma is not None:\n",
    "      inv *= gamma\n",
    "\n",
    "    # wrap conv kernel and bias with bn parameters\n",
    "    folded_kernel = inv * kernel\n",
    "    folded_bias = inv * (bias - moving_mean) + beta\n",
    "\n",
    "    return [folded_kernel, folded_bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3536efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_anomalous_data(file_name):\n",
    "    with h5py.File(file_name, 'r') as hf:\n",
    "        nmuon, nLRjet, nSRjet, negamma, netau, njtau = 4, 6, 6, 4, 4, 4\n",
    "\n",
    "        print(hf.keys())  # 可保留调试\n",
    "\n",
    "        def fix_phi_range(phi_data):\n",
    "            phi_fixed = phi_data % (2 * math.pi)\n",
    "            phi_scaled = np.round(phi_fixed * 128 / (2 * math.pi))\n",
    "            return phi_scaled % 128\n",
    "\n",
    "        # 命名统一为 load_and_scale_both_versions\n",
    "        def load_and_scale_both_versions(dataset, n_objects, scale_factor=10, eta_factor=40):\n",
    "            raw = hf[dataset][:, 0:n_objects, :]\n",
    "\n",
    "            scaled = np.copy(raw)\n",
    "            scaled[:, :, 0] *= scale_factor\n",
    "            scaled[:, :, 1] *= eta_factor\n",
    "            scaled[:, :, 2] = fix_phi_range(scaled[:, :, 2])\n",
    "\n",
    "            return (\n",
    "                scaled.reshape(-1, 3 * n_objects),\n",
    "                raw.reshape(-1, 3 * n_objects)\n",
    "            )\n",
    "\n",
    "        # 以下只加载 Topo 2A 所需部分\n",
    "        L1_jFexSR_jets, L1_jFexSR_jets_raw = load_and_scale_both_versions('L1_jFexSR_jets', nSRjet)\n",
    "        L1_eFex_taus, L1_eFex_taus_raw = load_and_scale_both_versions('L1_eFex_taus', netau)\n",
    "        L1_muons, L1_muons_raw = load_and_scale_both_versions('L1_muons', nmuon, scale_factor=10000)\n",
    "\n",
    "        # 处理 MET 数据（scaled 和 raw）\n",
    "        L1_MET = hf['L1_MET'][:]\n",
    "        L1_MET_raw = np.copy(L1_MET)\n",
    "\n",
    "        L1_MET[:, 0] *= 10\n",
    "        L1_MET[:, 2] = fix_phi_range(L1_MET[:, 2])\n",
    "        L1_MET_scaled = np.stack([L1_MET[:, 0], L1_MET[:, 2]], axis=1)\n",
    "\n",
    "        L1_MET_raw = np.stack([L1_MET_raw[:, 0], L1_MET_raw[:, 2]], axis=1)\n",
    "\n",
    "        # 只保留必要字段\n",
    "        pass_L1_unprescaled = hf[\"pass_L1_unprescaled\"][:]\n",
    "\n",
    "        # 合并 scaled 和 raw Topo 2A\n",
    "        Topo_2A = np.concatenate([L1_jFexSR_jets, L1_eFex_taus, L1_muons, L1_MET_scaled], axis=1)\n",
    "        Topo_2A_unscaled = np.concatenate([L1_jFexSR_jets_raw, L1_eFex_taus_raw, L1_muons_raw, L1_MET_raw], axis=1)\n",
    "\n",
    "        # 与 background 对齐的 fill_median\n",
    "        def fill_median(array):\n",
    "            for i in range(array.shape[1]):\n",
    "                col = array[:, i]\n",
    "                median = np.nanmedian(col)\n",
    "                array[np.isnan(col), i] = median\n",
    "            return array\n",
    "\n",
    "        Topo_2A = fill_median(Topo_2A)\n",
    "        Topo_2A_unscaled = fill_median(Topo_2A_unscaled)\n",
    "\n",
    "        return Topo_2A, Topo_2A_unscaled, pass_L1_unprescaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d145f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['EB_weights', 'HLT_MET_cell', 'HLT_MET_mhtpufit_pf', 'HLT_MET_mhtpufit_pf_subjesgscIS', 'HLT_MET_nn', 'HLT_MET_pfopufit', 'HLT_MET_tcpufit', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'averageInteractionsPerCrossing', 'event_number', 'lumiBlock', 'lumiblock_bads', 'mu', 'pass_HLT_unprescaled', 'pass_L1_unprescaled', 'run_number']>\n",
      "<KeysViewHDF5 ['EB_weights', 'HLT_MET_cell', 'HLT_MET_mhtpufit_pf', 'HLT_MET_mhtpufit_pf_subjesgscIS', 'HLT_MET_nn', 'HLT_MET_pfopufit', 'HLT_MET_tcpufit', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'averageInteractionsPerCrossing', 'event_number', 'lumiBlock', 'lumiblock_bads', 'mu', 'pass_HLT_unprescaled', 'pass_L1_unprescaled', 'run_number']>\n",
      "<KeysViewHDF5 ['EB_weights', 'HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'averageInteractionsPerCrossing', 'event_number', 'lumiBlock', 'lumiblock_bads', 'mu', 'pass_HLT_unprescaled', 'pass_L1_unprescaled', 'run_number']>\n",
      "<KeysViewHDF5 ['EB_weights', 'HLT_MET_cell', 'HLT_MET_mhtpufit_pf', 'HLT_MET_mhtpufit_pf_subjesgscIS', 'HLT_MET_nn', 'HLT_MET_pfopufit', 'HLT_MET_tcpufit', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'averageInteractionsPerCrossing', 'event_number', 'lumiBlock', 'lumiblock_bads', 'mu', 'pass_HLT_unprescaled', 'pass_L1_unprescaled', 'run_number']>\n",
      "<KeysViewHDF5 ['EB_weights', 'HLT_MET_cell', 'HLT_MET_mhtpufit_pf', 'HLT_MET_mhtpufit_pf_subjesgscIS', 'HLT_MET_nn', 'HLT_MET_pfopufit', 'HLT_MET_tcpufit', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'averageInteractionsPerCrossing', 'event_number', 'lumiBlock', 'lumiblock_bads', 'mu', 'pass_HLT_unprescaled', 'pass_L1_unprescaled', 'run_number']>\n"
     ]
    }
   ],
   "source": [
    "Topo_2A_jz1, Topo_2A_jz1_unscaled, pass_L1_jz1 = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/jjJZ1_07-17-2024.h5')\n",
    "Topo_2A_jz2_23e, Topo_2A_jz2_23e_unscaled, pass_L1_jz2_23e = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/04-15-2025/MC/MC_jjJZ2_04-15-2025.h5')\n",
    "Topo_2A_jz4_23e, Topo_2A_jz4_23e_unscaled, pass_L1_jz4_23e = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/04-15-2025/MC/MC_jjJZ4_04-15-2025.h5')\n",
    "Topo_2A_ttbar_1lep_23e, Topo_2A_ttbar_1lep_23e_unscaled, pass_L1_ttbar_1lep_23e = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/04-15-2025/misc/mc23e_ttbar_1lep.h5')\n",
    "Topo_2A_ttbar_2lep_23e, Topo_2A_ttbar_2lep_23e_unscaled, pass_L1_ttbar_2lep_23e = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/04-15-2025/MC/MC_ttbar_2lep_04-15-2025.h5')\n",
    "Topo_2A_ZZ4lep_23e, Topo_2A_ZZ4lep_23e_unscaled, pass_L1_ZZ4lep_23e = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/04-15-2025/MC/MC_ZZ4lep_04-15-2025.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b007396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "def Qmake_encoder_set_weights(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    l2_factor = 1e-3\n",
    "    inputs = keras.Input(shape=(input_dim), name = \"inputs\")\n",
    "    x = QDenseBatchnorm(h_dim_1,\n",
    "             kernel_quantizer=quantized_bits(10,5,0,alpha=1),\n",
    "             bias_quantizer=quantized_bits(10,5,0,alpha=1),\n",
    "             kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "             bias_initializer=keras.initializers.Zeros(),\n",
    "            name = \"dense1\")(inputs)\n",
    "    x = QActivation(quantized_relu(15, negative_slope=1/1024), name = 'relu1')(x)\n",
    "    x = QDenseBatchnorm(h_dim_2,\n",
    "             kernel_quantizer=quantized_bits(10,5,0,alpha=1),\n",
    "             bias_quantizer=quantized_bits(10,5,0,alpha=1),\n",
    "             kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "             bias_initializer=keras.initializers.Zeros(),\n",
    "            name = \"dense2\")(x)\n",
    "    x = QActivation(quantized_relu(15, negative_slope=1/1024), name = 'relu2')(x)\n",
    "    z_mean=QDense(latent_dim, name='z_mean',\n",
    "                  kernel_quantizer=quantized_bits(10,5,0,alpha=1),\n",
    "                  bias_quantizer=quantized_bits(10,5,0,alpha=1),\n",
    "                  kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                  bias_initializer=keras.initializers.Zeros())(x)\n",
    "    z_logvar=Dense(latent_dim, name='z_log_var',\n",
    "                          kernel_initializer=keras.initializers.Zeros(),\n",
    "                          bias_initializer=keras.initializers.Zeros())(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    encoder = keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "def Qmake_decoder_set_weights(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    l2_factor = 1e-3\n",
    "    inputs=keras.Input(shape=(latent_dim))\n",
    "    x=layers.Dense(h_dim_2,\n",
    "                   activation='relu',\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros())(inputs)\n",
    "    x = BatchNormalization(name=\"BN_decode1\")(x)\n",
    "    x=layers.Dense(h_dim_1,\n",
    "                   activation='relu',\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros())(x)\n",
    "    x = BatchNormalization(name=\"BN_decode2\")(x)\n",
    "    z=layers.Dense(input_dim,\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros())(x)\n",
    "    decoder=keras.Model(inputs,z,name='decoder')\n",
    "    return decoder\n",
    "def Qmake_discriminator(input_dim, h_dim_1, h_dim_2):\n",
    "    l2_factor = 1e-3\n",
    "    inputs = keras.Input(shape=(input_dim))\n",
    "    x = Dense(h_dim_1,\n",
    "              activation='relu',\n",
    "              kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "              bias_initializer=keras.initializers.Zeros())(inputs)\n",
    "    x = Dense(h_dim_2,\n",
    "              activation='relu',\n",
    "              kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "              bias_initializer=keras.initializers.Zeros())(x)\n",
    "    x = Dense(1,\n",
    "              activation='sigmoid',  # Output probability\n",
    "              kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "              bias_initializer=keras.initializers.Zeros())(x)\n",
    "    discriminator = keras.Model(inputs, x, name='discriminator')\n",
    "    return discriminator\n",
    "def custom_mse_loss_with_multi_index_scaling(masked_data, masked_reconstruction):\n",
    "    jet_scale = 1\n",
    "    tau_scale = 1\n",
    "    muon_sacle = 1\n",
    "    met_scale = 1\n",
    "\n",
    "    # Define the indices and their corresponding scale factors\n",
    "    scale_dict = {\n",
    "        0: jet_scale,\n",
    "        3: jet_scale,\n",
    "        6: jet_scale,\n",
    "        9: jet_scale,\n",
    "        12: jet_scale,\n",
    "        15: jet_scale,\n",
    "        18: tau_scale,\n",
    "        21: tau_scale,\n",
    "        24: tau_scale,\n",
    "        27: tau_scale,\n",
    "        30: muon_sacle,\n",
    "        33: muon_sacle,\n",
    "        36: muon_sacle,\n",
    "        39: muon_sacle,\n",
    "        42: met_scale\n",
    "    }\n",
    "    \n",
    "    # Create the scaling tensor\n",
    "    scale_tensor = tf.ones_like(masked_data)\n",
    "    \n",
    "    for index, factor in scale_dict.items():\n",
    "        index_mask = tf.one_hot(index, depth=tf.shape(masked_data)[-1])\n",
    "        scale_tensor += index_mask * (factor - 1)\n",
    "    \n",
    "    # Apply scaling\n",
    "    scaled_data = masked_data * scale_tensor\n",
    "    scaled_reconstruction = masked_reconstruction * scale_tensor\n",
    "    \n",
    "    ########\n",
    "    # Hardcoded lists for eta and phi indices\n",
    "    eta_indices = [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40]\n",
    "    phi_indices = [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 43]\n",
    "\n",
    "    batch_size = tf.shape(scaled_reconstruction)[0]\n",
    "    \n",
    "    # Apply constraints to eta\n",
    "    for i in eta_indices:\n",
    "        indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "        updates = 3 * tf.tanh(scaled_reconstruction[:, i] / 3)\n",
    "        scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    \n",
    "    # Apply constraints to phi\n",
    "    for i in phi_indices:\n",
    "        indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "        updates = 3.14159265258979*(10/8) * tf.tanh(scaled_reconstruction[:, i] / (3.14159265258979*(10/8)))\n",
    "        scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    #########\n",
    "\n",
    "    # Calculate MSE using keras.losses.mse\n",
    "    mse = keras.losses.mse(scaled_data, scaled_reconstruction)\n",
    "    \n",
    "    # Take the mean across all dimensions\n",
    "    return tf.reduce_mean(mse)\n",
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self, encoder, decoder, discriminator, steps_per_epoch=20, cycle_length=20, min_beta=0, max_beta=1, min_gamma=0, max_gamma=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder        \n",
    "        self.discriminator = discriminator\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "        self.gamma_tracker = keras.metrics.Mean(name=\"gamma\")\n",
    "        \n",
    "        self.beta_tracker = keras.metrics.Mean(name=\"beta\")\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.cycle_length = tf.cast(cycle_length, tf.float32)\n",
    "        self.min_beta = tf.cast(min_beta, tf.float32)\n",
    "        self.max_beta = tf.cast(max_beta, tf.float32)\n",
    "        self.beta = tf.Variable(min_beta, dtype=tf.float32)\n",
    "        self.min_gamma = tf.cast(min_gamma, tf.float32)\n",
    "        self.max_gamma = tf.cast(max_gamma, tf.float32)\n",
    "        self.gamma = tf.Variable(min_gamma, dtype=tf.float32)\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super(VAE_Model, self).compile(**kwargs)\n",
    "        # Set the optimizer for the entire model (encoder + decoder + discriminator)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Collect trainable variables from encoder, decoder, and discriminator\n",
    "        trainable_variables = (\n",
    "            self.encoder.trainable_weights + \n",
    "            self.decoder.trainable_weights + \n",
    "            self.discriminator.trainable_weights\n",
    "        )\n",
    "        # Build the optimizer with the full variable list\n",
    "        self.optimizer.build(trainable_variables)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.beta_tracker,\n",
    "        ]\n",
    "    def cyclical_annealing_beta(self, epoch):\n",
    "        cycle = tf.floor(1.0 + epoch / self.cycle_length)\n",
    "        x = tf.abs(epoch / self.cycle_length - cycle + 1)\n",
    "        # For first half (x < 0.5), scale 2x from 0 to 1\n",
    "        # For second half (x >= 0.5), stay at 1\n",
    "        scaled_x = tf.where(x < 0.5, 2.0 * x, 1.0)\n",
    "        return self.min_beta + (self.max_beta - self.min_beta) * scaled_x\n",
    "    def get_gamma_schedule(self, epoch):\n",
    "        # Convert to float32 for TF operations\n",
    "        epoch = tf.cast(epoch, tf.float32)\n",
    "        \n",
    "        # Calculate annealing progress\n",
    "        anneal_progress = (epoch - 50.0) / 50.0\n",
    "        gamma_anneal = self.min_gamma + (self.max_gamma - self.min_gamma) * anneal_progress\n",
    "        \n",
    "        # Implement the conditions using tf.where\n",
    "        gamma = tf.where(epoch < 50.0, \n",
    "                        0.0,  # if epoch < 50\n",
    "                        tf.where(epoch >= 100.0,\n",
    "                                self.max_gamma,  # if epoch >= 100\n",
    "                                gamma_anneal))   # if 50 <= epoch < 100\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Get the current epoch number\n",
    "        epoch = tf.cast(self.optimizer.iterations / self.steps_per_epoch, tf.float32)\n",
    "        \n",
    "        # Update beta\n",
    "        self.beta.assign(self.cyclical_annealing_beta(epoch))\n",
    "        self.gamma.assign(self.get_gamma_schedule(epoch))\n",
    "\n",
    "        # ---------------------------\n",
    "        # Train the Discriminator\n",
    "        # ---------------------------\n",
    "        with tf.GradientTape() as tape_disc:\n",
    "            # Generate reconstructed data\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Get discriminator predictions\n",
    "            real_output = self.discriminator(data)\n",
    "            fake_output = self.discriminator(reconstruction * mask)\n",
    "            \n",
    "            # Labels for real and fake data\n",
    "            real_labels = tf.ones_like(real_output)\n",
    "            fake_labels = tf.zeros_like(fake_output)\n",
    "            \n",
    "            # Discriminator loss\n",
    "            d_loss_real = keras.losses.binary_crossentropy(real_labels, real_output)\n",
    "            d_loss_fake = keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss = tf.reduce_mean(d_loss)\n",
    "        \n",
    "        grads_disc = tape_disc.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_weights))\n",
    "\n",
    "        # ---------------------------\n",
    "        # Train the VAE (Generator)\n",
    "        # ---------------------------\n",
    "        with tf.GradientTape() as tape:\n",
    "            data_noisy = data + tf.random.normal(tf.shape(data), \n",
    "                                               mean=0, \n",
    "                                               stddev=0)\n",
    "            # Keep the mask logic\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "            data_noisy = data_noisy * mask  # Apply mask to noisy data\n",
    "            \n",
    "            z_mean, z_log_var, z = self.encoder(data_noisy)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*data, mask*reconstruction)\n",
    "\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "            # Generator (VAE) wants to fool the discriminator\n",
    "            fake_output = self.discriminator(mask*reconstruction)\n",
    "            valid_labels = tf.ones_like(fake_output)  # Try to make discriminator think reconstructions are real\n",
    "            g_loss_adv = keras.losses.binary_crossentropy(valid_labels, fake_output)\n",
    "            g_loss_adv = tf.reduce_mean(g_loss_adv)\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss * self.beta + g_loss_adv * self.gamma\n",
    "            \n",
    "        grads_vae = tape.gradient(total_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads_vae, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.discriminator_loss_tracker.update_state(g_loss_adv)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reco_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"disc_loss\": self.discriminator_loss_tracker.result(),\n",
    "            \"beta\": self.beta,\n",
    "            \"raw_loss\": self.reconstruction_loss_tracker.result() + self.kl_loss_tracker.result(),\n",
    "            \"w_kl_loss\": self.kl_loss_tracker.result() * self.beta,\n",
    "            \"w_disc_loss\": self.discriminator_loss_tracker.result() * self.gamma,\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*data, mask*reconstruction)\n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        # Discriminator loss (only for monitoring)\n",
    "        # pass both data and reconstruction through D to get generator adversarial loss\n",
    "        real_output = self.discriminator(data)\n",
    "        fake_output = self.discriminator(mask*reconstruction)\n",
    "        real_labels = tf.ones_like(real_output)\n",
    "        fake_labels = tf.zeros_like(fake_output)\n",
    "        d_loss_real = keras.losses.binary_crossentropy(real_labels, real_output)\n",
    "        d_loss_fake = keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss = tf.reduce_mean(d_loss)\n",
    "        \n",
    "        # Generator adversarial loss\n",
    "        valid_labels = tf.ones_like(fake_output)\n",
    "        g_loss_adv = keras.losses.binary_crossentropy(valid_labels, fake_output)\n",
    "        g_loss_adv = tf.reduce_mean(g_loss_adv)\n",
    "        total_loss = reconstruction_loss + kl_loss * self.beta + g_loss_adv * self.gamma\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reco_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "            \"raw_loss\": reconstruction_loss + kl_loss,\n",
    "            \"disc_loss\": g_loss_adv,\n",
    "            \"w_kl_loss\": kl_loss * self.beta,\n",
    "            \"w_disc_loss\": g_loss_adv * self.gamma\n",
    "        }\n",
    "\n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c008a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Qmake_encoder_set_weights(44, 32, 16, 3)\n",
    "vae_dec = Qmake_decoder_set_weights(44, 32, 16, 3)\n",
    "vae_disc = Qmake_discriminator(44, 8, 2)\n",
    "vis_model = VAE_Model(encoder, vae_dec, vae_disc, steps_per_epoch=20, min_beta=0, max_beta=0.1, min_gamma=0.01, max_gamma=0.1)\n",
    "vis_model.load_weights('/eos/user/r/ruxie/calibration/L1AD_sw_model/versionFDL-GAN_ALT/best_kl_loss/').expect_partial()\n",
    "vis_encoder = vis_model.get_layer(\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e1cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [7, 5, 5, 7, 5, 5, 5, 4, 4, 5, 4, 3, 4, 3, 3, 3, 3, 2, 6, 5, 5, 5, 4, 5, 4, 3, 4, 3, 2, 3, 4, 4, 4, 2, 3, 3, 1, 1, 2, -2, -1, -1, 6, 5]\n",
    "def apply_power_of_2_scaling(X, result):\n",
    "    # Apply the scaling using 2 raised to the power of the result\n",
    "    X_scaled = X / (2.0 ** np.array(result))\n",
    "    return X_scaled\n",
    "Topo_2A_jz1 = apply_power_of_2_scaling(Topo_2A_jz1, result)\n",
    "Topo_2A_jz2_23e = apply_power_of_2_scaling(Topo_2A_jz2_23e, result)\n",
    "Topo_2A_jz4_23e = apply_power_of_2_scaling(Topo_2A_jz4_23e, result)\n",
    "Topo_2A_ttbar_1lep_23e = apply_power_of_2_scaling(Topo_2A_ttbar_1lep_23e, result)\n",
    "Topo_2A_ttbar_2lep_23e = apply_power_of_2_scaling(Topo_2A_ttbar_2lep_23e, result)\n",
    "Topo_2A_ZZ4lep_23e = apply_power_of_2_scaling(Topo_2A_ZZ4lep_23e, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c6d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_score_exact(encoder, events):\n",
    "    \"\"\"\n",
    "    Compute anomaly score exactly as in original code:\n",
    "    mean(mu^2) per event, where mu = encoder output[0].\n",
    "    \"\"\"\n",
    "    latent = encoder(events)       # returns [z_mean, z_logvar, z]\n",
    "    mus = latent[0].numpy()        # mu = z_mean\n",
    "    scores = np.mean(np.square(mus), axis=1)\n",
    "    return scores\n",
    "def find_anomalous_events_with_unscaled(signals_scaled, signals_unscaled, signal_names, encoder):\n",
    "    results = {}\n",
    "    for scaled, unscaled, name in zip(signals_scaled, signals_unscaled, signal_names):\n",
    "        scores = compute_anomaly_score_exact(encoder, scaled)\n",
    "        results[name] = {\n",
    "            \"scaled\": scaled,\n",
    "            \"unscaled\": unscaled,\n",
    "            \"scores\": scores\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55fe5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_scaled = [\n",
    "    Topo_2A_jz1,\n",
    "    Topo_2A_jz2_23e,\n",
    "    Topo_2A_jz4_23e,\n",
    "    Topo_2A_ttbar_1lep_23e,\n",
    "    Topo_2A_ttbar_2lep_23e,\n",
    "    Topo_2A_ZZ4lep_23e\n",
    "]\n",
    "\n",
    "signals_unscaled = [\n",
    "    Topo_2A_jz1_unscaled,\n",
    "    Topo_2A_jz2_23e_unscaled,\n",
    "    Topo_2A_jz4_23e_unscaled,\n",
    "    Topo_2A_ttbar_1lep_23e_unscaled,\n",
    "    Topo_2A_ttbar_2lep_23e_unscaled,\n",
    "    Topo_2A_ZZ4lep_23e_unscaled\n",
    "]\n",
    "\n",
    "signal_names = [\n",
    "    'jz1',\n",
    "    'jz2_23e',\n",
    "    'jz4_23e',\n",
    "    'ttbar_1lep_23e',\n",
    "    'ttbar_2lep_23e',\n",
    "    'ZZ4lep_23e'\n",
    "]\n",
    "\n",
    "events = find_anomalous_events_with_unscaled(signals_scaled, signals_unscaled, signal_names, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17f10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = []\n",
    "for i in range(6):\n",
    "    feature_names += [f'jFexSR_jet{i}_pt (GeV)', f'jFexSR_jet{i}_eta', f'jFexSR_jet{i}_phi (rad)']\n",
    "for i in range(4):\n",
    "    feature_names += [f'eFex_tau{i}_pt (GeV)', f'eFex_tau{i}_eta', f'eFex_tau{i}_phi (rad)']\n",
    "for i in range(4):\n",
    "    feature_names += [f'muon{i}_pt (GeV)', f'muon{i}_eta', f'muon{i}_phi (rad)']\n",
    "feature_names += ['MET_pt (GeV)', 'MET_phi (rad)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5899a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_500Hz = 1521991 / (256 * 256 * 3)\n",
    "threshold_1kHz  = 1333204 / (256 * 256 * 3)\n",
    "threshold_2kHz = 1237997 / (256 * 256 * 3)\n",
    "threshold_3kHz = 1156836 / (256 * 256 * 3)\n",
    "threshold_4kHz = 1096728 / (256 * 256 * 3)\n",
    "threshold_5kHz = 1051755 / (256 * 256 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439567c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "def plot_anomaly_vs_kinematics(events, feature_names, save_dir):\n",
    "    \"\"\"\n",
    "    events: dict, output of find_anomalous_events_with_unscaled\n",
    "    feature_names: list of str\n",
    "    save_dir: str, main folder to save plots\n",
    "    \"\"\"\n",
    "    # different signals in different colors\n",
    "    colors = itertools.cycle(plt.cm.tab10.colors) \n",
    "    \n",
    "    for signal_name, color in zip(events.keys(), colors):\n",
    "        data = events[signal_name]\n",
    "        unscaled = data[\"unscaled\"]\n",
    "        scores = data[\"scores\"]\n",
    "\n",
    "        # a subfolder per process\n",
    "        signal_dir = os.path.join(save_dir, signal_name) \n",
    "        os.makedirs(signal_dir, exist_ok=True)\n",
    "        \n",
    "        for i, feature in enumerate(feature_names):\n",
    "            x = unscaled[:, i]\n",
    "            y = scores\n",
    "\n",
    "            plt.figure(figsize=(8, 6))  # increase fig size\n",
    "            \n",
    "            # Create 2D histogram with 50 bins in each dimension, using log scale for counts\n",
    "            h = plt.hist2d(x, y, bins=50, cmap='viridis', cmin=1, norm=LogNorm())\n",
    "            cbar = plt.colorbar(h[3], label='Log(Counts)')\n",
    "            \n",
    "            # exclude pt=0 for linear regression\n",
    "            mask = x != 0\n",
    "            if np.sum(mask) > 2:  # at least 3 data\n",
    "                slope, intercept, r_value, p_value, std_err = linregress(x[mask], y[mask])\n",
    "                x_fit = np.linspace(np.min(x[mask]), np.max(x[mask]), 100)\n",
    "                y_fit = slope * x_fit + intercept\n",
    "                plt.plot(x_fit, y_fit, color=\"red\", linestyle=\"-\", linewidth=1.5,\n",
    "                         label=f\"Linear fit (r={r_value:.3f})\")\n",
    "                x_fit_ext = np.linspace(0, np.min(x[mask]), 100)\n",
    "                y_fit_ext = slope * x_fit_ext + intercept\n",
    "                plt.plot(x_fit_ext, y_fit_ext, color=\"red\", linestyle=\"--\", linewidth=1.2)\n",
    "            \n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel(\"Anomaly Score\")\n",
    "            plt.title(f\"{feature} vs Anomaly Score ({signal_name})\")\n",
    "\n",
    "            threshold_lines = [\n",
    "                (threshold_500Hz, 'red', '500Hz'),\n",
    "                (threshold_1kHz, 'blue', '1kHz'),\n",
    "                (threshold_2kHz, 'purple', '2kHz'),\n",
    "                (threshold_3kHz, 'teal', '3kHz'),\n",
    "                (threshold_4kHz, 'darkorange', '4kHz'),\n",
    "                (threshold_5kHz, 'seagreen', '5kHz')\n",
    "            ]\n",
    "            \n",
    "            for threshold, color_val, label in threshold_lines:\n",
    "                plt.axhline(threshold, color=color_val, linestyle='--', linewidth=1.0, alpha=0.8)\n",
    "\n",
    "            from matplotlib.lines import Line2D\n",
    "            legend_elements = [\n",
    "                Line2D([0], [0], color='red', linestyle='-', linewidth=1.5, label=f'Linear fit (r={r_value:.3f})' if np.sum(mask) > 2 else ''),\n",
    "                *[Line2D([0], [0], color=color_val, linestyle='--', linewidth=1.0, label=label) \n",
    "                  for threshold, color_val, label in threshold_lines]\n",
    "            ]\n",
    "            \n",
    "            legend_elements = [elem for elem in legend_elements if elem.get_label()]\n",
    "            \n",
    "            plt.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98), \n",
    "                      framealpha=0.9, fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            file_path = os.path.join(signal_dir, f\"{feature}.png\")\n",
    "            plt.savefig(file_path, dpi=150, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "save_dir=\"0918_anomaly_vs_kinematics_legend_aside\"\n",
    "plot_anomaly_vs_kinematics(events, feature_names, save_dir)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac8aa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_efficiency_vs_kinematics(events, feature_names, save_dir, bins):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        events: dict, output of find_anomalous_events_with_unscaled\n",
    "        feature_names: list of str\n",
    "        save_dir: str, main folder to save plots\n",
    "        bins: int, number of bins\n",
    "    \"\"\"\n",
    "    \n",
    "    for process_name, data in events.items():\n",
    "        unscaled = data[\"unscaled\"]\n",
    "        scores = data[\"scores\"]\n",
    "        \n",
    "        process_dir = os.path.join(save_dir, process_name)\n",
    "        os.makedirs(process_dir, exist_ok=True)\n",
    "        \n",
    "        for i, feature in enumerate(feature_names):\n",
    "            values = unscaled[:, i]\n",
    "            \n",
    "            bin_edges = np.linspace(values.min(), values.max(), bins+1)\n",
    "            bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "            \n",
    "            pass_rate_500 = []\n",
    "            pass_rate_1k  = []\n",
    "            pass_rate_2k  = []\n",
    "            pass_rate_3k  = []\n",
    "            pass_rate_4k  = []\n",
    "            pass_rate_5k  = []\n",
    "            \n",
    "            # record bins with events\n",
    "            has_data_mask = []\n",
    "            \n",
    "            for j in range(bins):\n",
    "                mask = (values >= bin_edges[j]) & (values < bin_edges[j+1])\n",
    "                if np.any(mask):\n",
    "                    pass_rate_500.append(np.mean(scores[mask] > threshold_500Hz))\n",
    "                    pass_rate_1k.append(np.mean(scores[mask] > threshold_1kHz))\n",
    "                    pass_rate_2k.append(np.mean(scores[mask] > threshold_2kHz))\n",
    "                    pass_rate_3k.append(np.mean(scores[mask] > threshold_3kHz))\n",
    "                    pass_rate_4k.append(np.mean(scores[mask] > threshold_4kHz))\n",
    "                    pass_rate_5k.append(np.mean(scores[mask] > threshold_5kHz))\n",
    "                    has_data_mask.append(True)\n",
    "                else:\n",
    "                    pass_rate_500.append(np.nan)\n",
    "                    pass_rate_1k.append(np.nan)\n",
    "                    pass_rate_2k.append(np.nan)\n",
    "                    pass_rate_3k.append(np.nan)\n",
    "                    pass_rate_4k.append(np.nan)\n",
    "                    pass_rate_5k.append(np.nan)\n",
    "                    has_data_mask.append(False)\n",
    "            \n",
    "            bin_centers = np.array(bin_centers)\n",
    "            pass_rate_500 = np.array(pass_rate_500)\n",
    "            pass_rate_1k = np.array(pass_rate_1k)\n",
    "            pass_rate_2k = np.array(pass_rate_2k)\n",
    "            pass_rate_3k = np.array(pass_rate_3k)\n",
    "            pass_rate_4k = np.array(pass_rate_4k)\n",
    "            pass_rate_5k = np.array(pass_rate_5k)\n",
    "            has_data_mask = np.array(has_data_mask)\n",
    "            \n",
    "            plt.figure(figsize=(8,6))\n",
    "            \n",
    "            # only link dots representing bins with events\n",
    "            def plot_with_data_mask(x, y, label, color, **kwargs):\n",
    "                data_indices = np.where(has_data_mask)[0]\n",
    "                if len(data_indices) == 0:\n",
    "                    return\n",
    "                \n",
    "                segments = []\n",
    "                current_segment = [data_indices[0]]\n",
    "                for i in range(1, len(data_indices)):\n",
    "                    if data_indices[i] == data_indices[i-1] + 1:\n",
    "                        current_segment.append(data_indices[i])\n",
    "                    else:\n",
    "                        segments.append(current_segment)\n",
    "                        current_segment = [data_indices[i]]\n",
    "                segments.append(current_segment)\n",
    "                \n",
    "                # 为每个连续段画线（不绘制标记点）\n",
    "                for segment in segments:\n",
    "                    if len(segment) > 1:  # only link if at least 2 dots\n",
    "                        plt.plot(x[segment], y[segment], color=color, label=label, **kwargs)\n",
    "                    elif len(segment) == 1:  # 单个点也画线（长度为1的线）\n",
    "                        plt.plot(x[segment], y[segment], color=color, label=label, **kwargs)\n",
    "                \n",
    "                # 移除重复的图例标签\n",
    "                handles, labels = plt.gca().get_legend_handles_labels()\n",
    "                by_label = dict(zip(labels, handles))\n",
    "                plt.gca().legend(by_label.values(), by_label.keys())\n",
    "            \n",
    "            # 绘制各阈值曲线（去掉marker参数）\n",
    "            plot_with_data_mask(bin_centers, pass_rate_500, label=\"500Hz\", color=\"#f4ae6f\", linewidth=1.5, alpha=0.8, linestyle='-')\n",
    "            plot_with_data_mask(bin_centers, pass_rate_1k, label=\"1kHz\", color=\"#99c9db\", linewidth=1.5, alpha=0.8, linestyle='-')\n",
    "            plot_with_data_mask(bin_centers, pass_rate_2k, label=\"2kHz\", color=\"#c82423\", linewidth=1.5, alpha=0.8, linestyle='-')\n",
    "            plot_with_data_mask(bin_centers, pass_rate_3k, label=\"3kHz\", color=\"#6668ae\", linewidth=1.5, alpha=0.8, linestyle='-')\n",
    "            plot_with_data_mask(bin_centers, pass_rate_4k, label=\"4kHz\", color=\"#fa7f6f\", linewidth=1.5, alpha=0.8, linestyle='-')\n",
    "            plot_with_data_mask(bin_centers, pass_rate_5k, label=\"5kHz\", color=\"#2878b4\", linewidth=1.5, alpha=0.8, linestyle='-')\n",
    "            \n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel(\"Trigger Efficiency\")\n",
    "            plt.ylim(0, 1.05)\n",
    "            plt.title(f\"Efficiency vs {feature} ({process_name})\")\n",
    "            \n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            file_path = os.path.join(process_dir, f\"{feature}_Efficiency.png\")\n",
    "            plt.savefig(file_path, dpi=150, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "save_dir=\"0918_efficiency_vs_kinematics_10bins\"\n",
    "bins=10\n",
    "plot_efficiency_vs_kinematics(events, feature_names, save_dir, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a336fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def heatmap_anomaly_vs_jetcount(events, save_dir, bins_score=50):\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     jet_pt_indices = [i*3 for i in range(6)] \n",
    "\n",
    "#     for process_name, data in events.items():\n",
    "#         unscaled = data[\"unscaled\"]\n",
    "#         scores = data[\"scores\"]\n",
    "\n",
    "#         jet_counts = np.sum(unscaled[:, jet_pt_indices] > 0.0, axis=1) # count jet number\n",
    "\n",
    "#         x_bins = np.arange(jet_counts.min()-0.5, jet_counts.max()+1.5, 1)  # jet count is discrete\n",
    "#         y_bins = np.linspace(scores.min(), scores.max(), bins_score)\n",
    "\n",
    "#         hist, x_edges, y_edges = np.histogram2d(jet_counts, scores, bins=[x_bins, y_bins])\n",
    "\n",
    "#         hist_log = np.log1p(hist.T)\n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(6,5))\n",
    "#         im = ax.imshow(hist_log, origin=\"lower\", aspect=\"auto\",\n",
    "#                        extent=[x_edges[0], x_edges[-1], y_edges[0], y_edges[-1]],\n",
    "#                        cmap=\"viridis\")\n",
    "#         cbar = plt.colorbar(im, ax=ax)\n",
    "#         cbar.set_label(\"log(Event Count + 1)\")\n",
    "\n",
    "#         nx, ny = hist_log.shape  # nx = score bins, ny = jet count bins after transpose\n",
    "#         for ix in range(ny):\n",
    "#             for iy in range(nx):\n",
    "#                 val = hist_log[iy, ix]\n",
    "#                 count = hist.T[iy, ix]\n",
    "#                 if count > 0:\n",
    "#                     x_text = (x_edges[ix] + x_edges[ix+1]) / 2\n",
    "#                     y_text = (y_edges[iy] + y_edges[iy+1]) / 2\n",
    "#                     ax.text(x_text, y_text, f\"{val:.1f}\",\n",
    "#                             color=\"white\" if val > hist_log.max()/2 else \"black\",\n",
    "#                             ha=\"center\", va=\"center\", fontsize=7)\n",
    "\n",
    "#         plt.axhline(threshold_500Hz, color='red', linestyle='--', label='Threshold 500Hz')\n",
    "#         plt.axhline(threshold_1kHz, color='blue', linestyle='--', label='Threshold 1kHz')\n",
    "#         plt.axhline(threshold_2kHz, color='purple', linestyle='--', label='Threshold 2kHz')\n",
    "#         plt.axhline(threshold_3kHz, color='teal', linestyle='--', label='Threshold 3kHz')\n",
    "#         plt.axhline(threshold_4kHz, color='darkorange', linestyle='--', label='Threshold 4kHz')\n",
    "#         plt.axhline(threshold_5kHz, color='seagreen', linestyle='--', label='Threshold 5kHz')\n",
    "#         ax.legend()\n",
    "\n",
    "#         ax.set_xlabel(f\"Jet Count (pt > 0)\")\n",
    "#         ax.set_ylabel(\"Anomaly Score\")\n",
    "#         ax.set_title(f\"Anomaly Score vs Jet Count ({process_name})\")\n",
    "\n",
    "#         plt.savefig(os.path.join(save_dir, f\"{process_name}.png\"), dpi=300)\n",
    "#         plt.close()\n",
    "# save_dir=\"0827_anomaly_vs_jetcount\"\n",
    "# heatmap_anomaly_vs_jetcount(events, save_dir, bins_score=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e479fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def heatmap_anomaly_vs_bijet_invariant_mass(events, save_dir, bins_score=25):\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     jet0_idx = [0, 1, 2]\n",
    "#     jet1_idx = [3, 4, 5]\n",
    "\n",
    "#     def calc_mjj(jet0, jet1):\n",
    "#         pt0, eta0, phi0 = jet0\n",
    "#         pt1, eta1, phi1 = jet1\n",
    "\n",
    "#         # E = pt*cosh(eta), px=pt*cos(phi), py=pt*sin(phi), pz=pt*sinh(eta)\n",
    "#         E0 = pt0 * np.cosh(eta0)\n",
    "#         px0 = pt0 * np.cos(phi0)\n",
    "#         py0 = pt0 * np.sin(phi0)\n",
    "#         pz0 = pt0 * np.sinh(eta0)\n",
    "\n",
    "#         E1 = pt1 * np.cosh(eta1)\n",
    "#         px1 = pt1 * np.cos(phi1)\n",
    "#         py1 = pt1 * np.sin(phi1)\n",
    "#         pz1 = pt1 * np.sinh(eta1)\n",
    "\n",
    "#         E_tot = E0 + E1\n",
    "#         px_tot = px0 + px1\n",
    "#         py_tot = py0 + py1\n",
    "#         pz_tot = pz0 + pz1\n",
    "\n",
    "#         mjj2 = E_tot**2 - px_tot**2 - py_tot**2 - pz_tot**2\n",
    "#         mjj2 = np.maximum(mjj2, 0)\n",
    "#         return np.sqrt(mjj2)\n",
    "\n",
    "#     for process_name, data in events.items():\n",
    "#         unscaled = data[\"unscaled\"]\n",
    "#         scores = data[\"scores\"]\n",
    "\n",
    "#         # 提取 leading jets 并计算 mjj\n",
    "#         jet0 = unscaled[:, jet0_idx]\n",
    "#         jet1 = unscaled[:, jet1_idx]\n",
    "#         mjj = np.array([calc_mjj(j0, j1) for j0, j1 in zip(jet0, jet1)])\n",
    "\n",
    "#         # 定义 bin\n",
    "#         x_bins = np.linspace(mjj.min(), mjj.max(), 50)\n",
    "#         y_bins = np.linspace(scores.min(), scores.max(), bins_score)\n",
    "\n",
    "#         # 2D 直方图\n",
    "#         hist, x_edges, y_edges = np.histogram2d(mjj, scores, bins=[x_bins, y_bins])\n",
    "\n",
    "#         hist_log = np.log1p(hist.T)\n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(6,5))\n",
    "#         im = ax.imshow(hist_log, origin=\"lower\", aspect=\"auto\",\n",
    "#                        extent=[x_edges[0], x_edges[-1], y_edges[0], y_edges[-1]],\n",
    "#                        cmap=\"viridis\")\n",
    "#         cbar = plt.colorbar(im, ax=ax)\n",
    "#         cbar.set_label(\"log(Event Count + 1)\")\n",
    "\n",
    "#         # 显示每个 bin 的数值\n",
    "#         # nx, ny = hist_log.shape\n",
    "#         # for ix in range(ny):\n",
    "#         #     for iy in range(nx):\n",
    "#         #         val = hist_log[iy, ix]\n",
    "#         #         count = hist.T[iy, ix]\n",
    "#         #         if count > 0:\n",
    "#         #             x_text = (x_edges[ix] + x_edges[ix+1]) / 2\n",
    "#         #             y_text = (y_edges[iy] + y_edges[iy+1]) / 2\n",
    "#         #             ax.text(x_text, y_text, f\"{val:.1f}\",\n",
    "#         #                     color=\"white\" if val > hist_log.max()/2 else \"black\",\n",
    "#         #                     ha=\"center\", va=\"center\", fontsize=7)\n",
    "\n",
    "#         plt.axhline(threshold_500Hz, color='red', linestyle='--', label='Threshold 500Hz')\n",
    "#         plt.axhline(threshold_1kHz, color='blue', linestyle='--', label='Threshold 1kHz')\n",
    "#         plt.axhline(threshold_2kHz, color='purple', linestyle='--', label='Threshold 2kHz')\n",
    "#         plt.axhline(threshold_3kHz, color='teal', linestyle='--', label='Threshold 3kHz')\n",
    "#         plt.axhline(threshold_4kHz, color='darkorange', linestyle='--', label='Threshold 4kHz')\n",
    "#         plt.axhline(threshold_5kHz, color='seagreen', linestyle='--', label='Threshold 5kHz')\n",
    "#         ax.legend()\n",
    "\n",
    "#         ax.set_xlabel(\"Invariant Mass of Leading Jets (GeV)\")\n",
    "#         ax.set_ylabel(\"Anomaly Score\")\n",
    "#         ax.set_title(f\"Anomaly Score vs Leading Jets Mjj ({process_name})\")\n",
    "\n",
    "#         plt.savefig(os.path.join(save_dir, f\"{process_name}.png\"), dpi=300)\n",
    "#         plt.close()\n",
    "# save_dir=\"0830_anomaly_vs_bijet_mass\"\n",
    "# heatmap_anomaly_vs_bijet_invariant_mass(events, save_dir, bins_score=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095234a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_anomaly_vs_muoncount_heatmap(events, save_dir, pt_threshold=0.0, bins_score=50):\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     # 每个 muon 的 pt 在 unscaled 中的位置 (4个 muons, 每个3个特征: pt, eta, phi)\n",
    "#     muon_pt_indices = [18 + i*3 for i in range(4)]  \n",
    "\n",
    "#     for process_name, data in events.items():\n",
    "#         unscaled = data[\"unscaled\"]\n",
    "#         scores = data[\"scores\"]\n",
    "\n",
    "#         # 计算 muon 数量\n",
    "#         muon_counts = np.sum(unscaled[:, muon_pt_indices] > pt_threshold, axis=1)\n",
    "\n",
    "#         # 定义 bin\n",
    "#         x_bins = np.arange(muon_counts.min()-0.5, muon_counts.max()+1.5, 1)  # muon count 离散\n",
    "#         y_bins = np.linspace(scores.min(), scores.max(), bins_score)\n",
    "\n",
    "#         # 2D 直方图\n",
    "#         hist, x_edges, y_edges = np.histogram2d(muon_counts, scores, bins=[x_bins, y_bins])\n",
    "\n",
    "#         # 转置用于 imshow\n",
    "#         hist_log = np.log1p(hist.T)\n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(6,5))\n",
    "#         im = ax.imshow(hist_log, origin=\"lower\", aspect=\"auto\",\n",
    "#                        extent=[x_edges[0], x_edges[-1], y_edges[0], y_edges[-1]],\n",
    "#                        cmap=\"viridis\")\n",
    "#         cbar = plt.colorbar(im, ax=ax)\n",
    "#         cbar.set_label(\"log(Event Count + 1)\")\n",
    "\n",
    "#         # 显示每个 bin 的数值\n",
    "#         nx, ny = hist_log.shape\n",
    "#         for ix in range(ny):\n",
    "#             for iy in range(nx):\n",
    "#                 val = hist_log[iy, ix]\n",
    "#                 count = hist.T[iy, ix]\n",
    "#                 if count > 0:\n",
    "#                     x_text = (x_edges[ix] + x_edges[ix+1]) / 2\n",
    "#                     y_text = (y_edges[iy] + y_edges[iy+1]) / 2\n",
    "#                     ax.text(x_text, y_text, f\"{val:.1f}\",\n",
    "#                             color=\"white\" if val > hist_log.max()/2 else \"black\",\n",
    "#                             ha=\"center\", va=\"center\", fontsize=7)\n",
    "\n",
    "#         # 画阈值水平线\n",
    "#         ax.axhline(threshold_500Hz, color='red', linestyle='--', label='500Hz threshold')\n",
    "#         ax.axhline(threshold_1kHz, color='orange', linestyle='--', label='1kHz threshold')\n",
    "#         ax.legend()\n",
    "\n",
    "#         ax.set_xlabel(f\"Muon Count (pt > {pt_threshold:.1f})\")\n",
    "#         ax.set_ylabel(\"Anomaly Score\")\n",
    "#         ax.set_title(f\"Anomaly Score vs Muon Count ({process_name})\")\n",
    "\n",
    "#         plt.savefig(os.path.join(save_dir, f\"{process_name}.png\"), dpi=300)\n",
    "#         plt.close()\n",
    "# save_dir=\"anomaly_vs_muoncount_heatmap\"\n",
    "# plot_anomaly_vs_muoncount_heatmap(events, save_dir, pt_threshold=0.0, bins_score=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
